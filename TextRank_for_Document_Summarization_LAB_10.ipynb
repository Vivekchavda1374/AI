{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyb/OnG9UgiVrdjZDfNNU2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivekchavda1374/AI/blob/main/TextRank_for_Document_Summarization_LAB_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T-eAUpL52Kj4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary nltk resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xTkxiVy2SQ5",
        "outputId": "8d9cefc4-0c1f-4cfa-b8bc-414847464d04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_frequencies = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = word_tokenize(sent.lower())\n",
        "        filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "        word_frequencies.append(Counter(filtered_words))\n",
        "\n",
        "    return sentences, word_frequencies"
      ],
      "metadata": {
        "id": "NwwP0ChY2V_A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_matrix(word_frequencies):\n",
        "    size = len(word_frequencies)\n",
        "    similarity_matrix = np.zeros((size, size))\n",
        "\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            if i != j:\n",
        "                words1 = word_frequencies[i]\n",
        "                words2 = word_frequencies[j]\n",
        "                common_words = set(words1.keys()).union(set(words2.keys()))\n",
        "\n",
        "                vec1 = np.array([words1[word] for word in common_words])\n",
        "                vec2 = np.array([words2[word] for word in common_words])\n",
        "\n",
        "                similarity_matrix[i][j] = cosine_similarity([vec1], [vec2])[0, 0]\n",
        "\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "BoDJACFG2YGh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank_summarization(text, num_sentences=3):\n",
        "    sentences, word_frequencies = preprocess_text(text)\n",
        "    similarity_matrix = build_similarity_matrix(word_frequencies)\n",
        "\n",
        "    # Build graph and rank sentences using PageRank\n",
        "    graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(graph)\n",
        "\n",
        "    # Sort sentences by score and select top-ranked ones\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    summary = \" \".join([sent for _, sent in ranked_sentences[:num_sentences]])\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "HlKPJblv2bIp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "text = \"\"\"TextRank is an unsupervised algorithm for keyword extraction and text summarization.\n",
        "It is based on PageRank, which is used by Google to rank web pages in search results.\n",
        "TextRank builds a graph of sentences, where edges represent similarity between them.\n",
        "By running the PageRank algorithm on this graph, we can extract the most important sentences\n",
        "for summarization. This technique is widely used in NLP applications.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBf_-_Ja2dAx",
        "outputId": "8220ae81-ab55-4ea4-815f-be58aa38889d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = textrank_summarization(text, num_sentences=2)\n",
        "print(\"Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIxX4oSs2el5",
        "outputId": "52bfd28b-03f1-4a66-b8a5-cd4de6ade940"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "By running the PageRank algorithm on this graph, we can extract the most important sentences\n",
            "for summarization. TextRank is an unsupervised algorithm for keyword extraction and text summarization.\n"
          ]
        }
      ]
    }
  ]
}